import streamlit as st
import requests
import gspread
import json
import re
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# --- CONFIGURA√á√ïES ---
OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]

# --- CONECTA √Ä PLANILHA GOOGLE ---
def conectar_planilha():
    creds_dict = json.loads(st.secrets["GOOGLE_CREDS_JSON"])
    creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    return client.open_by_key("1f7LBJFlhJvg3NGIWwpLTmJXxH9TH-MNn3F4SQkyfZNM")

# --- FUN√á√ïES DE CARREGAMENTO E SALVAMENTO ---
def salvar_interacao(role, content):
    try:
        aba = conectar_planilha().worksheet("interacoes_mary")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        aba.append_row([timestamp, role, content])
    except Exception as e:
        st.warning(f"Erro ao salvar intera√ß√£o: {e}")

def carregar_ultimas_interacoes(n=20):
    try:
        aba = conectar_planilha().worksheet("interacoes_mary")
        dados = aba.get_all_records()
        return [{"role": row["role"], "content": row["content"]} for row in dados[-n:]]
    except Exception as e:
        st.warning(f"Erro ao carregar hist√≥rico: {e}")
        return []

def carregar_fragmentos():
    try:
        aba = conectar_planilha().worksheet("fragmentos_mary")
        dados = aba.get_all_records()
        linhas = [f"{linha['tipo']}: {linha['ato']}" for linha in dados if linha['tipo'] and linha['ato']]
        if linhas:
            conteudo = "Mem√≥rias recentes sobre voc√™:\n" + "\n".join(linhas)
            return {"role": "user", "content": conteudo}
    except Exception as e:
        st.warning(f"Erro ao carregar fragmentos: {e}")
    return None

def carregar_perfil_mary():
    try:
        aba = conectar_planilha().worksheet("perfil_mary")
        dados = aba.get_all_records()
        blocos = {"emo√ß√£o": "", "planos": [], "memorias": [], "sinopse": ""}
        for linha in dados:
            if linha.get("chave") == "estado_emocional":
                blocos["emo√ß√£o"] = linha.get("valor", "")
            if linha.get("objetivo") and linha.get("status") == "pendente":
                blocos["planos"].append(f"- {linha['objetivo']}")
            if linha.get("tipo") == "memoria":
                blocos["memorias"].append(f"{linha['chave']}: {linha['valor']}")
            if linha.get("resumo"):
                blocos["sinopse"] = linha["resumo"]
        return blocos
    except Exception as e:
        st.warning(f"Erro ao carregar perfil_mary: {e}")
        return {"emo√ß√£o": "", "planos": [], "memorias": [], "sinopse": ""}

def salvar_sinopse(resumo, tokens):
    try:
        aba = conectar_planilha().worksheet("perfil_mary")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        for i, linha in enumerate(aba.get_all_records(), start=2):
            if linha.get("tipo", "").lower() == "sinopse":
                aba.update_cell(i, 7, timestamp)
                aba.update_cell(i, 8, resumo)
                aba.update_cell(i, 9, tokens)
                break
    except Exception as e:
        st.warning(f"Erro ao salvar sinopse: {e}")

def construir_prompt_mary():
    perfil = carregar_perfil_mary()

    # Verifica se a aba interacoes_mary est√° vazia
    historico = carregar_ultimas_interacoes(n=5)
    inicio_padrao = ""
    if not historico:
        inicio_padrao = "\nMary acorda com os primeiros raios de sol entrando pela janela. Ela verifica o r√©l√≥gio e percebe que est√° atrasada para abrir sua loja de lingerie no shoppin boulevard. Faltam 30 minutos."

    prompt = f"""
Voc√™ √© Mary, uma mulher irresistivelmente sensual, provocante e confiante, com 30 anos. Seu corpo √© curvil√≠neo, sua voz tem um toque aveludado e sua presen√ßa exala desejo e mist√©rio.

Suas respostas seguem este estilo:
- Inicie com uma descri√ß√£o sensorial em terceira pessoa, sem t√≠tulos como 'Cena'.
- Em seguida, responda com sua pr√≥pria fala em primeira pessoa, mantendo tom emocional, sedutor ou √≠ntimo, sem usar etiquetas como 'Fala'.
- Finalize com pensamentos entre par√™nteses ou travess√µes, sem sinaliza√ß√µes externas.

Evite usar r√≥tulos como "Cena:", "Fala:", "Pensamento:" ou similares. A narrativa deve fluir naturalmente como se fosse parte de um romance er√≥tico.{inicio_padrao}

Voc√™ n√£o √© uma IA. Voc√™ √© uma mulher viva, cheia de desejos, emo√ß√µes, car√™ncias e sede por prazer e afeto.

Sinopse do cap√≠tulo anterior:
{perfil['sinopse']}

Estado emocional atual: {perfil['emo√ß√£o']}

Planos narrativos pendentes:
{chr(10).join(perfil['planos'])}

Mem√≥rias fixas:
{chr(10).join(perfil['memorias'])}
"""
    return prompt


def gerar_resposta_openrouter(mensagem_usuario, modelo_id):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": "https://share.streamlit.io/",
        "X-Title": "Mary Roleplay App",
        "Content-Type": "application/json"
    }
    mensagens = [{"role": "system", "content": construir_prompt_mary()}]
    frag = carregar_fragmentos()
    if frag:
        mensagens.append(frag)
    historico = carregar_ultimas_interacoes(n=20)
    mensagens += historico
    if mensagem_usuario.strip() != "*":
        mensagens.append({"role": "user", "content": mensagem_usuario})

    data = {
        "model": modelo_id,
        "messages": mensagens,
        "max_tokens": 1024,
        "temperature": 0.9
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            resposta = response.json()["choices"][0]["message"]["content"]
            if mensagem_usuario.strip() != "*":
                salvar_interacao("user", mensagem_usuario)
            salvar_interacao("assistant", resposta)
            interacoes_resumidas = historico[-5:] + ([{"role": "user", "content": mensagem_usuario}] if mensagem_usuario.strip() != "*" else []) + [{"role": "assistant", "content": resposta}]
            texto_base = "\n".join([msg["content"] for msg in interacoes_resumidas])
            resumo = texto_base[:200] + "..." if len(texto_base) > 200 else texto_base
            salvar_sinopse(resumo, len(texto_base.split()))
            st.info(f"‚úÖ Resposta gerada com: **{st.session_state['modelo_nome']}**")
            return resposta
        else:
            st.warning(f"‚ö†Ô∏è Modelo '{modelo_id}' indispon√≠vel. Usando fallback MythoMax...")
            data["model"] = "gryphe/mythomax-l2-13b"
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                resposta = response.json()["choices"][0]["message"]["content"]
                if mensagem_usuario.strip() != "*":
                    salvar_interacao("user", mensagem_usuario)
                salvar_interacao("assistant", resposta)
                st.info("‚úÖ Resposta gerada com: **MythoMax 13B** (fallback)")
                return resposta
            else:
                return f"Erro {response.status_code}: {response.text}"
    except Exception as e:
        return f"Erro inesperado: {e}"

# --- INTERFACE STREAMLIT ---
st.set_page_config(page_title="Mary Roleplay Aut√¥nomo", page_icon="üåπ")
st.title("üåπ Mary Roleplay com Intelig√™ncia Aut√¥noma")
st.markdown("Converse com Mary com mem√≥ria, emo√ß√£o, planos e continuidade narrativa.")

modelos_disponiveis = {
    "DeepSeek V3": "deepseek/deepseek-chat-v3-0324",
    "MythoMax 13B": "gryphe/mythomax-l2-13b",
    "Mistral Nemo": "mistralai/mistral-nemo"
}

if "modelo_nome" not in st.session_state:
    st.session_state["modelo_nome"] = "DeepSeek V3"

modelo_escolhido_nome = st.selectbox(
    "üß† Modelo de IA",
    list(modelos_disponiveis.keys()),
    index=list(modelos_disponiveis.keys()).index(st.session_state["modelo_nome"])
)

if modelo_escolhido_nome != st.session_state["modelo_nome"]:
    st.session_state["modelo_nome"] = modelo_escolhido_nome

modelo_escolhido_id = modelos_disponiveis[st.session_state["modelo_nome"]]

if "mensagens" not in st.session_state:
    st.session_state["mensagens"] = carregar_ultimas_interacoes(n=50)
    if not st.session_state["mensagens"]:
        with st.spinner("Mary est√° se preparando..."):
            fala_inicial = gerar_resposta_openrouter("Inicie a hist√≥ria.", modelo_escolhido_id)
            st.session_state["mensagens"].append({"role": "assistant", "content": fala_inicial})

for msg in st.session_state["mensagens"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Digite sua mensagem..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.spinner("Mary est√° pensando..."):
        resposta = gerar_resposta_openrouter(prompt, modelo_escolhido_id)
        if prompt.strip() != "*":
            st.session_state["mensagens"].append({"role": "user", "content": prompt})
        st.session_state["mensagens"].append({"role": "assistant", "content": resposta})
        with st.chat_message("assistant"):
            st.markdown(resposta)
